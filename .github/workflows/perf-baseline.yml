name: perf-baseline

on:
  pull_request:
    paths:
      - "SPPPy/**"
      - "tests/**"
      - "README.md"
      - "requirements.txt"
      - "pytest.ini"
      - ".github/workflows/perf-baseline.yml"
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: ubuntu-22.04
    env:
      RUN_PERF_BASELINE: "1"
      PERF_BENCH_ROUNDS: "10"
      PERF_BENCH_WARMUP_ROUNDS: "2"
      PERF_THETA_STEP_DEG: "0.2"
      PERF_WL_STEP_NM: "2.0"
      PERF_H_STEP_NM: "2.0"
      PERF_PAR_WORKERS: "2"
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-benchmark line_profiler

      - name: Run baseline benchmarks
        run: |
          mkdir -p artifacts/benchmark
          pytest tests/test_performance_baseline.py \
            --benchmark-only \
            --benchmark-json artifacts/benchmark/benchmark.json

      - name: Convert benchmark report to CSV
        run: |
          python tests/benchmark_to_csv.py \
            --benchmark-json artifacts/benchmark/benchmark.json \
            --csv-output artifacts/benchmark/benchmark_summary.csv


      - name: Build benchmark comparison summary
        run: |
          python tests/benchmark_compare.py \
            --benchmark-json artifacts/benchmark/benchmark.json \
            --summary-json artifacts/benchmark/benchmark_compare.json \
            --summary-md artifacts/benchmark/benchmark_compare.md
          cat artifacts/benchmark/benchmark_compare.md >> "$GITHUB_STEP_SUMMARY"

      - name: Run profiling
        run: |
          python tests/profile_baseline.py --output-dir artifacts/profiling

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: perf-baseline-artifacts
          path: artifacts/
